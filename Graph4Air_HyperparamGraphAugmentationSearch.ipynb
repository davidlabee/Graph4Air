{"cells":[{"cell_type":"code","execution_count":1,"id":"pc7vp-j75qWf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58176,"status":"ok","timestamp":1748943214040,"user":{"displayName":"David Labee","userId":"02476639796914504484"},"user_tz":-120},"id":"pc7vp-j75qWf","outputId":"86714989-a196-4190-d38f-e687ada2d656"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m378.9/386.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["import torch\n","\n","!pip install optuna -q\n","!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y -q\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html -q\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html -q\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html -q\n","!pip install git+https://github.com/pyg-team/pytorch_geometric.git -q"]},{"cell_type":"code","execution_count":3,"id":"vcm8eveu5pxB","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7694,"status":"ok","timestamp":1748943730233,"user":{"displayName":"David Labee","userId":"02476639796914504484"},"user_tz":-120},"id":"vcm8eveu5pxB","outputId":"8efe6970-13fc-42bd-b994-0e19a8342159"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# ────────────────────────────────────────────────────────────\n","# 1) Core Imports & Drive Mount\n","# ────────────────────────────────────────────────────────────\n","import numpy as np\n","import pandas as pd\n","import geopandas as gpd\n","import os\n","\n","import networkx as nx\n","\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from scipy.stats import pearsonr\n","\n","from tqdm.auto import tqdm\n","import random\n","from collections import Counter\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv, GATConv\n","\n","import optuna\n","from optuna.samplers import TPESampler\n","from optuna.pruners import HyperbandPruner\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"id":"da64352d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":863},"id":"da64352d","outputId":"fc227711-3c80-412b-ce4a-21e48e8dfc70","executionInfo":{"status":"error","timestamp":1748944222530,"user_tz":-120,"elapsed":492311,"user":{"displayName":"David Labee","userId":"02476639796914504484"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-06-03 09:43:37,258] A new study created in memory with name: no-name-284fe83c-0a6e-491f-92ea-d2c1590928c1\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ Detected 2172 outlier segments.\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-06-03 09:44:19,065] Trial 0 finished with value: 9.361431016999227 and parameters: {'top_n': 1500, 'neighbors': 140, 'sim_thresh': 0.9045428335139303, 'min_dist': 362, 'max_dist': 1300, 'hop_thresh': 1, 'max_edges': 2000, 'per_node_cap': 6}. Best is trial 0 with value: 9.361431016999227.\n","[I 2025-06-03 09:44:46,202] Trial 1 finished with value: 9.14056406897427 and parameters: {'top_n': 500, 'neighbors': 200, 'sim_thresh': 0.9400459152127869, 'min_dist': 174, 'max_dist': 2068, 'hop_thresh': 3, 'max_edges': 3500, 'per_node_cap': 7}. Best is trial 1 with value: 9.14056406897427.\n","[I 2025-06-03 09:45:10,743] Trial 2 finished with value: 9.516810752013956 and parameters: {'top_n': 500, 'neighbors': 60, 'sim_thresh': 0.9041314051664475, 'min_dist': 196, 'max_dist': 4348, 'hop_thresh': 2, 'max_edges': 1500, 'per_node_cap': 2}. Best is trial 1 with value: 9.14056406897427.\n","[I 2025-06-03 09:45:37,355] Trial 3 finished with value: 9.301831989943736 and parameters: {'top_n': 500, 'neighbors': 140, 'sim_thresh': 0.9093570006461043, 'min_dist': 371, 'max_dist': 3297, 'hop_thresh': 4, 'max_edges': 500, 'per_node_cap': 5}. Best is trial 1 with value: 9.14056406897427.\n","[I 2025-06-03 09:46:12,747] Trial 4 finished with value: 9.324696177424203 and parameters: {'top_n': 1500, 'neighbors': 110, 'sim_thresh': 0.8295301485960922, 'min_dist': 467, 'max_dist': 2061, 'hop_thresh': 5, 'max_edges': 4000, 'per_node_cap': 2}. Best is trial 1 with value: 9.14056406897427.\n","[I 2025-06-03 09:46:32,506] Trial 5 finished with value: 9.664149734979155 and parameters: {'top_n': 1000, 'neighbors': 10, 'sim_thresh': 0.9683947908015637, 'min_dist': 339, 'max_dist': 1392, 'hop_thresh': 3, 'max_edges': 3000, 'per_node_cap': 8}. Best is trial 1 with value: 9.14056406897427.\n","[I 2025-06-03 09:46:55,089] Trial 6 finished with value: 9.54584803575249 and parameters: {'top_n': 1000, 'neighbors': 50, 'sim_thresh': 0.8234575244562391, 'min_dist': 207, 'max_dist': 510, 'hop_thresh': 2, 'max_edges': 3000, 'per_node_cap': 2}. Best is trial 1 with value: 9.14056406897427.\n","[I 2025-06-03 09:47:35,346] Trial 7 finished with value: 9.046087306490062 and parameters: {'top_n': 1500, 'neighbors': 190, 'sim_thresh': 0.8918426528443748, 'min_dist': 360, 'max_dist': 3372, 'hop_thresh': 4, 'max_edges': 500, 'per_node_cap': 5}. Best is trial 7 with value: 9.046087306490062.\n","[I 2025-06-03 09:48:12,679] Trial 8 finished with value: 9.396955492177913 and parameters: {'top_n': 1000, 'neighbors': 180, 'sim_thresh': 0.9371613386645665, 'min_dist': 408, 'max_dist': 4673, 'hop_thresh': 2, 'max_edges': 2500, 'per_node_cap': 5}. Best is trial 7 with value: 9.046087306490062.\n","[I 2025-06-03 09:48:36,341] Trial 9 finished with value: 9.210949096154701 and parameters: {'top_n': 500, 'neighbors': 90, 'sim_thresh': 0.8291695084357477, 'min_dist': 282, 'max_dist': 4744, 'hop_thresh': 4, 'max_edges': 1500, 'per_node_cap': 3}. Best is trial 7 with value: 9.046087306490062.\n","[I 2025-06-03 09:49:14,687] Trial 10 finished with value: 9.473269551666688 and parameters: {'top_n': 1500, 'neighbors': 160, 'sim_thresh': 0.8662259270441812, 'min_dist': 55, 'max_dist': 839, 'hop_thresh': 5, 'max_edges': 5000, 'per_node_cap': 10}. Best is trial 7 with value: 9.046087306490062.\n","[I 2025-06-03 09:49:53,142] Trial 11 finished with value: 9.39534494916856 and parameters: {'top_n': 1000, 'neighbors': 200, 'sim_thresh': 0.9621109552791223, 'min_dist': 131, 'max_dist': 2321, 'hop_thresh': 4, 'max_edges': 4000, 'per_node_cap': 8}. Best is trial 7 with value: 9.046087306490062.\n","[I 2025-06-03 09:50:18,650] Trial 12 finished with value: 9.116853186485224 and parameters: {'top_n': 500, 'neighbors': 190, 'sim_thresh': 0.9924323157056077, 'min_dist': 259, 'max_dist': 2673, 'hop_thresh': 3, 'max_edges': 500, 'per_node_cap': 7}. Best is trial 7 with value: 9.046087306490062.\n","[W 2025-06-03 09:50:22,310] Trial 13 failed with parameters: {'top_n': 1500, 'neighbors': 160, 'sim_thresh': 0.8684137325428497, 'min_dist': 279, 'max_dist': 2661, 'hop_thresh': 4, 'max_edges': 500, 'per_node_cap': 4} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","                      ^^^^^^^^^^^\n","  File \"<ipython-input-4-1be5ea88b1ac>\", line 293, in objective_gcn\n","    G_aug, _ = augment_grouped_far_knn(\n","               ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<ipython-input-4-1be5ea88b1ac>\", line 117, in augment_grouped_far_knn\n","    u, v = sorted((int(src), int(dst)))\n","                   ^^^^^^^^\n","KeyboardInterrupt\n","[W 2025-06-03 09:50:22,314] Trial 13 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1be5ea88b1ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;31m# Run GCN study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0mstudy_gcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m \u001b[0mstudy_gcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_gcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0mbest_params_gcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy_gcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔍 Best augmentation params for GCN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params_gcn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-1be5ea88b1ac>\u001b[0m in \u001b[0;36mobjective_gcn\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m# Build augmented graph & Data object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     G_aug, _ = augment_grouped_far_knn(\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-1be5ea88b1ac>\u001b[0m in \u001b[0;36maugment_grouped_far_knn\u001b[0;34m(G, gdf, groups, coords, feature_matrix, feature_cols, top_n, neighbors, sim_thresh, min_dist, max_dist, hop_thresh, max_edges, per_node_cap, road_id_col, suffix)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdst_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mroad_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mroad_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# ────────────────────────────────────────────────────────────\n","# 2) Data Prep & Graph + Outlier Detection\n","# ────────────────────────────────────────────────────────────\n","\n","# 2.1) Load GeoJSON into GeoDataFrame, project to EPSG:28992\n","fp = '/content/drive/MyDrive/Universiteit Utrecht/Thesis/data/road_network_lufeature.geojson'\n","gdf = gpd.read_file(fp).to_crs(epsg=28992).reset_index(drop=True)\n","gdf['centroid'] = gdf.geometry.centroid\n","coords = np.column_stack([gdf.centroid.x, gdf.centroid.y])\n","\n","# 2.2) Define target and feature columns\n","target_col = 'NO2d'\n","feature_cols = [\n","    c for c in gdf.columns\n","    if any(s in c for s in ['AGRI','INDUS','NATUR','PORT','RES','TRANS','URBG','WATER',\n","                             'POP','EEA','HHOLD','RDL','TLOA','HLOA','MRDL','TMLOA','HMLOA',\n","                             'TRAF','DINV'])\n","]\n","feature_matrix = gdf[feature_cols].to_numpy()\n","\n","# 2.3) Build initial NetworkX graph (nodes = segments, edges = touching)\n","G = nx.Graph()\n","sidx = gdf.sindex\n","for idx, row in gdf.iterrows():\n","    G.add_node(idx, **row.drop('geometry').to_dict())\n","for idx, geom in enumerate(gdf.geometry):\n","    for j in sidx.intersection(geom.bounds):\n","        if idx != j and geom.touches(gdf.geometry[j]):\n","            G.add_edge(idx, j)\n","\n","# 2.4) Mark highways for adaptive threshold\n","gdf['is_highway'] = gdf['TRAFMAJOR'] > 20000\n","group_thresh = {True: 9.0, False: 5.0}\n","\n","# 2.5) Outlier detection function\n","def detect_outliers(G, gdf, target_col='NO2d', hop=1):\n","    \"\"\"\n","    Detect spatial outliers based on 1-hop neighborhood residuals.\n","    Returns a list of node indices flagged as outliers.\n","    \"\"\"\n","    nodes = list(G.nodes())\n","    vals, neigh_means = [], []\n","    for n in nodes:\n","        v = gdf.at[n, target_col]\n","        if pd.isna(v):\n","            vals.append(np.nan); neigh_means.append(np.nan)\n","            continue\n","        sp = nx.single_source_shortest_path_length(G, n, cutoff=hop)\n","        neigh = [m for m in sp if m != n]\n","        nbr_vals = gdf.loc[neigh, target_col].dropna().values\n","        vals.append(v)\n","        neigh_means.append(np.nan if nbr_vals.size == 0 else nbr_vals.mean())\n","    vals = np.array(vals); neigh_means = np.array(neigh_means)\n","    valid = ~np.isnan(vals) & ~np.isnan(neigh_means)\n","    residuals = vals[valid] - neigh_means[valid]\n","    valid_nodes = np.array(nodes)[valid]\n","    med = np.median(residuals)\n","    mad = np.median(np.abs(residuals - med))\n","    # compute per-node cutoff\n","    cutoffs = np.array([\n","        group_thresh.get(gdf.at[n, 'is_highway'], 3.0) * mad\n","        for n in valid_nodes\n","    ])\n","    is_outlier = np.abs(residuals - med) > cutoffs\n","    return valid_nodes[is_outlier].tolist()\n","\n","# 2.6) Run outlier detection\n","error_segs = detect_outliers(G, gdf)\n","print(f\"⚠️ Detected {len(error_segs)} outlier segments.\")\n","\n","# ────────────────────────────────────────────────────────────\n","# 3) Graph Augmentation + Data Builder\n","# ────────────────────────────────────────────────────────────\n","\n","def augment_grouped_far_knn(\n","    G, gdf, groups, coords, feature_matrix, feature_cols,\n","    top_n, neighbors, sim_thresh, min_dist, max_dist,\n","    hop_thresh, max_edges, per_node_cap,\n","    road_id_col=\"ROAD_FID\", suffix='grp_far_knn'\n","):\n","    \"\"\"\n","    1) For each group, pick the top_n segments by intensity.\n","    2) In cosine feature space, find `neighbors` nearest peers with sim ≥ sim_thresh.\n","    3) Filter pairs by spatial distance [min_dist, max_dist], graph-hop > hop_thresh,\n","       different ROAD_FID.\n","    4) Enforce per-node cap and global max_edges.\n","    Returns: (augmented Graph, list of new edges).\n","    \"\"\"\n","    road_ids = gdf[road_id_col].to_numpy()\n","    col_to_idx = {c:i for i,c in enumerate(feature_cols)}\n","    candidates = set()\n","\n","    for cols in groups.values():\n","        # skip if any group feature missing\n","        if any(c not in col_to_idx for c in cols): continue\n","        intensity = gdf[cols].sum(axis=1)\n","        top_idx   = intensity.nlargest(top_n).index.to_numpy()\n","        if top_idx.size < 2: continue\n","\n","        idxs = [col_to_idx[c] for c in cols]\n","        subF = feature_matrix[top_idx][:, idxs]\n","        subF /= np.linalg.norm(subF, axis=1, keepdims=True).clip(1e-6)\n","\n","        nbr = NearestNeighbors(\n","            n_neighbors=min(neighbors+1, len(top_idx)),\n","            metric='cosine', n_jobs=-1\n","        ).fit(subF)\n","        dists, nn_idxs = nbr.kneighbors(subF)\n","        sims = 1 - dists  # cosine similarity\n","\n","        for ii, src in enumerate(top_idx):\n","            close = set(nx.single_source_shortest_path_length(G, int(src), cutoff=hop_thresh))\n","            for rank, dst_j in enumerate(nn_idxs[ii,1:], start=1):\n","                if sims[ii, rank] < sim_thresh:\n","                    break\n","                dst = top_idx[dst_j]\n","                u, v = sorted((int(src), int(dst)))\n","                if road_ids[src] == road_ids[dst]:\n","                    continue\n","                dxy = np.hypot(*(coords[src] - coords[dst]))\n","                if dxy < min_dist or dxy > max_dist or dst in close:\n","                    continue\n","                candidates.add((u, v))\n","\n","    # Prune to budgets\n","    final, counts = [], Counter()\n","    for u, v in random.sample(list(candidates), len(candidates)):\n","        if counts[u] < per_node_cap and counts[v] < per_node_cap:\n","            final.append((u, v))\n","            counts[u] += 1; counts[v] += 1\n","        if len(final) >= max_edges:\n","            break\n","\n","    G2 = G.copy()\n","    G2.add_edges_from(final, feature_sim=suffix)\n","    return G2, final\n","\n","def build_data_mask_missing(G, gdf, feature_cols, target_col, outliers=None):\n","    \"\"\"\n","    Constructs a PyG Data object from a NetworkX graph + GeoDataFrame.\n","    - Standardizes features\n","    - Builds edge_index (bi-directional)\n","    - Creates train/test masks (80/20 split), excluding any `outliers` from train.\n","    \"\"\"\n","    gdf2 = gdf.reset_index(drop=True)\n","    G2 = nx.relabel_nodes(G, {old:new for new,old in enumerate(gdf.index)})\n","\n","    X = StandardScaler().fit_transform(gdf2[feature_cols].values)\n","    y = gdf2[target_col].values.reshape(-1,1)\n","\n","    edges = np.array(list(G2.edges())).T\n","    edge_index = torch.tensor(\n","        np.concatenate([edges, edges[::-1]], axis=1),\n","        dtype=torch.long\n","    )\n","\n","    data = Data(\n","        x=torch.tensor(X, dtype=torch.float),\n","        edge_index=edge_index,\n","        y=torch.tensor(y, dtype=torch.float)\n","    )\n","\n","    valid_idx = np.where(~np.isnan(y.flatten()))[0]\n","    perm      = torch.randperm(len(valid_idx))\n","    n_train   = int(0.8 * len(valid_idx))\n","    train_idx = valid_idx[perm[:n_train].numpy()]\n","    test_idx  = valid_idx[perm[n_train:].numpy()]\n","\n","    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n","    test_mask  = torch.zeros(data.num_nodes, dtype=torch.bool)\n","    train_mask[train_idx] = True\n","    test_mask[test_idx]  = True\n","\n","    if outliers is not None:\n","        train_mask[outliers] = False\n","\n","    data.train_mask = train_mask\n","    data.test_mask  = test_mask\n","    return data\n","\n","# 3.1) Define groups used for augmentation\n","groups = {\n","    'industrial':        ['INDUS_300','INDUS_1000'],\n","    'residential':       ['RES_300','RES_1000'],\n","    'agriculture':       ['AGRI_300','AGRI_1000'],\n","    'natural':           ['NATUR_300','NATUR_1000'],\n","    'port':              ['PORT_300','PORT_1000'],\n","    'urb_built':         ['URBG_300','URBG_1000'],\n","    'water':             ['WATER_300','WATER_1000'],\n","    'traffic':           ['TRAFNEAR','TRAFMAJOR'],\n","    'pop':               ['POP_300','POP_1000'],\n","    'population_density':['EEA_300','EEA_1000'],\n","}\n","\n","# ────────────────────────────────────────────────────────────\n","# 4) Utility functions: eval_rmse & train_and_eval (revised)\n","# ────────────────────────────────────────────────────────────\n","\n","def eval_rmse(model, data):\n","    model.eval()\n","    with torch.no_grad():\n","        pred = model(data.x, data.edge_index)[data.test_mask]\\\n","               .cpu().numpy().flatten()\n","        true = data.y[data.test_mask]\\\n","               .cpu().numpy().flatten()\n","    return np.sqrt(mean_squared_error(true, pred))\n","\n","def train_and_eval(model_cls, data, device, model_init_args, optim_args):\n","    \"\"\"\n","    Instantiate `model_cls(**model_init_args)`, train for 50 epochs using optimizer settings\n","    from `optim_args`, and return test RMSE.\n","    \"\"\"\n","    model = model_cls(**model_init_args).to(device)\n","    data  = data.to(device)\n","    optimizer = torch.optim.Adam(\n","        model.parameters(),\n","        lr=optim_args['learning_rate'],\n","        weight_decay=optim_args['weight_decay']\n","    )\n","    for _ in range(50):\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(data.x, data.edge_index)\n","        loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","    return eval_rmse(model, data)\n","\n","# ────────────────────────────────────────────────────────────\n","# 5) Define Baseline GCN & GAT classes\n","# ────────────────────────────────────────────────────────────\n","\n","# GCN Model Definition (baseline)\n","class GCN(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super().__init__()\n","        self.conv1 = GCNConv(in_channels, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n","        self.conv3 = GCNConv(hidden_channels, out_channels)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv3(x, edge_index)\n","        x = F.relu(x)\n","        return x\n","\n","# GAT Model Definition (unchanged)\n","class GAT(nn.Module):\n","    def __init__(self, in_c, h_c, out_c, heads=2):\n","        super().__init__()\n","        self.g1 = GATConv(in_c,   h_c,  heads=heads)\n","        self.g2 = GATConv(h_c*heads, h_c, heads=heads)\n","        self.g3 = GATConv(h_c*heads, out_c, heads=1, concat=False)\n","\n","    def forward(self, x, e):\n","        x = F.elu(self.g1(x,e))\n","        x = F.elu(self.g2(x,e))\n","        return self.g3(x,e)\n","\n","# ────────────────────────────────────────────────────────────\n","# 6) Optuna Tuning: GCN first, then GAT\n","# ────────────────────────────────────────────────────────────\n","\n","# 6.1) Objective for GCN: tune only graph‐augmentation parameters,\n","#      with baseline GCN hyperparameters fixed\n","fixed_gcn_init = {\n","    'in_channels': len(feature_cols),\n","    'hidden_channels': 64,\n","    'out_channels': 1\n","}\n","fixed_gcn_optim = {\n","    'learning_rate': 0.009136733981799275,\n","    'weight_decay': 4.291139762395118e-05\n","}\n","\n","def objective_gcn(trial):\n","    # Propose new graph‐augmentation parameters\n","    aug_params = {\n","        'top_n':        trial.suggest_int('top_n',        500, 1500, step=500),\n","        'neighbors':    trial.suggest_int('neighbors',    10, 200,   step=10),\n","        'sim_thresh':   trial.suggest_float('sim_thresh', 0.80, 0.9999),\n","        'min_dist':     trial.suggest_int('min_dist',     50,  500),\n","        'max_dist':     trial.suggest_int('max_dist',     500, 5000, log=True),\n","        'hop_thresh':   trial.suggest_int('hop_thresh',   1,    5),\n","        'max_edges':    trial.suggest_int('max_edges',    500, 5000, step=500),\n","        'per_node_cap': trial.suggest_int('per_node_cap',  1,   10),\n","    }\n","\n","    # Build augmented graph & Data object\n","    G_aug, _ = augment_grouped_far_knn(\n","        G, gdf, groups, coords,\n","        feature_matrix, feature_cols,\n","        **aug_params\n","    )\n","    data = build_data_mask_missing(\n","        G_aug, gdf, feature_cols, target_col,\n","        outliers=error_segs\n","    )\n","\n","    # Train & evaluate using baseline GCN args + optimizer\n","    return train_and_eval(\n","        GCN, data,\n","        torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","        fixed_gcn_init, fixed_gcn_optim\n","    )\n","\n","# Run GCN study\n","study_gcn = optuna.create_study(direction='minimize', sampler=TPESampler())\n","study_gcn.optimize(objective_gcn, n_trials=200)\n","best_params_gcn = study_gcn.best_params\n","print(\"🔍 Best augmentation params for GCN:\", best_params_gcn)\n","\n","# 6.2) Consolidate best graph params for GCN\n","best_graph_params_gcn = {\n","    'top_n':        best_params_gcn['top_n'],\n","    'neighbors':    best_params_gcn['neighbors'],\n","    'sim_thresh':   best_params_gcn['sim_thresh'],\n","    'min_dist':     best_params_gcn['min_dist'],\n","    'max_dist':     best_params_gcn['max_dist'],\n","    'hop_thresh':   best_params_gcn['hop_thresh'],\n","    'max_edges':    best_params_gcn['max_edges'],\n","    'per_node_cap': best_params_gcn['per_node_cap']\n","}\n","\n","# 6.3) Objective for GAT: tune augmentation except top_n (use same top_n as GCN)\n","fixed_gat_init = {\n","    'in_c': len(feature_cols),\n","    'h_c': 16,\n","    'out_c': 1,\n","    'heads': 2\n","}\n","fixed_gat_optim = {\n","    'learning_rate': 0.009136733981799275,\n","    'weight_decay': 4.291139762395118e-05\n","}\n","\n","def objective_gat(trial):\n","    aug_params = {\n","        'top_n':        best_graph_params_gcn['top_n'],\n","        'neighbors':    trial.suggest_int('neighbors',    10, 200,   step=10),\n","        'sim_thresh':   trial.suggest_float('sim_thresh', 0.80, 0.9999),\n","        'min_dist':     trial.suggest_int('min_dist',     50,  500),\n","        'max_dist':     trial.suggest_int('max_dist',     500, 5000, log=True),\n","        'hop_thresh':   trial.suggest_int('hop_thresh',   1,    5),\n","        'max_edges':    trial.suggest_int('max_edges',    500, 5000, step=500),\n","        'per_node_cap': trial.suggest_int('per_node_cap',  1,   10),\n","    }\n","\n","    # Build augmented graph & Data object\n","    G_aug, _ = augment_grouped_far_knn(\n","        G, gdf, groups, coords,\n","        feature_matrix, feature_cols,\n","        **aug_params\n","    )\n","    data = build_data_mask_missing(\n","        G_aug, gdf, feature_cols, target_col,\n","        outliers=error_segs\n","    )\n","\n","    # Train & evaluate using baseline GAT args + optimizer\n","    return train_and_eval(\n","        lambda in_c, h_c, out_c, heads: GAT(in_c, h_c, out_c, heads),\n","        data,\n","        torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","        fixed_gat_init,\n","        fixed_gat_optim\n","    )\n","\n","# Run GAT study\n","study_gat = optuna.create_study(direction='minimize', sampler=TPESampler())\n","study_gat.optimize(objective_gat, n_trials=200)\n","best_params_gat_partial = study_gat.best_params\n","print(\"🔍 Best augmentation params (except top_n) for GAT:\", best_params_gat_partial)\n","\n","# 6.4) Consolidate best graph params for GAT\n","best_graph_params_gat = {\n","    'top_n':        best_graph_params_gcn['top_n'],\n","    'neighbors':    best_params_gat_partial['neighbors'],\n","    'sim_thresh':   best_params_gat_partial['sim_thresh'],\n","    'min_dist':     best_params_gat_partial['min_dist'],\n","    'max_dist':     best_params_gat_partial['max_dist'],\n","    'hop_thresh':   best_params_gat_partial['hop_thresh'],\n","    'max_edges':    best_params_gat_partial['max_edges'],\n","    'per_node_cap': best_params_gat_partial['per_node_cap']\n","}\n","\n","# ────────────────────────────────────────────────────────────\n","# 7) External Validation & Final Predictions\n","# ────────────────────────────────────────────────────────────\n","\n","# 7.1) Load Palmes measurements\n","palmes_fp  = '/content/drive/MyDrive/Universiteit Utrecht/Thesis/data/road_palmes_25m.geojson'\n","palmes_gdf = gpd.read_file(palmes_fp).to_crs(gdf.crs)\n","\n","def eval_external(params, model_cls, model_init, optim_init):\n","    \"\"\"\n","    Re‐augment graph, build data (excluding errors),\n","    train model_cls(**model_init) for 200 epochs with optim_init,\n","    then aggregate predictions at Palmes tubes and return RMSE.\n","    \"\"\"\n","    # a) Augment graph\n","    G_aug, _ = augment_grouped_far_knn(\n","        G, gdf, groups, coords,\n","        feature_matrix, feature_cols,\n","        **params\n","    )\n","    # b) Build Data object\n","    data_aug = build_data_mask_missing(\n","        G_aug, gdf, feature_cols, target_col,\n","        outliers=error_segs\n","    ).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","\n","    # c) Train fresh model\n","    model = model_cls(**model_init).to(data_aug.x.device)\n","    optimizer = torch.optim.Adam(\n","        model.parameters(),\n","        lr=optim_init['learning_rate'],\n","        weight_decay=optim_init['weight_decay']\n","    )\n","    for _ in range(200):\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(data_aug.x, data_aug.edge_index)\n","        loss = F.mse_loss(out[data_aug.train_mask], data_aug.y[data_aug.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","\n","    # d) Predict on all nodes\n","    model.eval()\n","    with torch.no_grad():\n","        preds = model(data_aug.x, data_aug.edge_index).detach().cpu().numpy().flatten()\n","\n","    # e) Aggregate at Palmes locations\n","    tmp = gdf.copy()\n","    tmp['pred'] = preds\n","    actuals, pred_means = [], []\n","    for _, row in palmes_gdf.iterrows():\n","        pt = row.geometry\n","        cands = list(gdf.sindex.intersection(pt.buffer(50).bounds))\n","        if cands:\n","            dists = tmp.loc[cands, 'geometry'].distance(pt).values\n","            near = [cands[i] for i, d in enumerate(dists) if d <= 50]\n","        else:\n","            near = []\n","        if not near:\n","            near = [tmp.geometry.distance(pt).idxmin()]\n","        pred_means.append(tmp.loc[near, 'pred'].mean())\n","        actuals.append(row['mean_annual_palmes_no2'])\n","    return np.sqrt(mean_squared_error(actuals, pred_means))\n","\n","# 7.2) External GCN RMSE\n","print(\"▶ External GCN RMSE:\",\n","      eval_external(best_graph_params_gcn, GCN,\n","                    {'in_channels': len(feature_cols), 'hidden_channels': 64, 'out_channels': 1},\n","                    {'learning_rate': 0.009136733981799275, 'weight_decay': 4.291139762395118e-05}))\n","\n","# 7.3) External GAT RMSE\n","print(\"▶ External GAT RMSE:\",\n","      eval_external(best_graph_params_gat, GAT,\n","                    {'in_c': len(feature_cols), 'h_c': 16, 'out_c': 1, 'heads': 2},\n","                    {'learning_rate': 0.009136733981799275, 'weight_decay': 4.291139762395118e-05}))\n","\n","# 7.4) Final retrain & save predictions on road segments\n","for name, (params, model_ctor, model_init, optim_init) in [\n","    ('GCN', (best_graph_params_gcn, GCN,\n","             {'in_channels': len(feature_cols), 'hidden_channels': 64, 'out_channels': 1},\n","             {'learning_rate': 0.009136733981799275, 'weight_decay': 4.291139762395118e-05})),\n","    ('GAT', (best_graph_params_gat, lambda in_c,h_c,out_c,heads: GAT(in_c,h_c,out_c,heads),\n","             {'in_c': len(feature_cols), 'h_c': 16, 'out_c': 1, 'heads': 2},\n","             {'learning_rate': 0.009136733981799275, 'weight_decay': 4.291139762395118e-05}))\n","]:\n","    G_fin, _ = augment_grouped_far_knn(\n","        G, gdf, groups, coords,\n","        feature_matrix, feature_cols,\n","        **params\n","    )\n","    data_fin = build_data_mask_missing(\n","        G_fin, gdf, feature_cols, target_col,\n","        outliers=error_segs\n","    ).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","    # Train final model and collect predictions\n","    model = model_ctor(**model_init).to(data_fin.x.device)\n","    optimizer = torch.optim.Adam(\n","        model.parameters(),\n","        lr=optim_init['learning_rate'],\n","        weight_decay=optim_init['weight_decay']\n","    )\n","    for _ in range(200):\n","        model.train()\n","        optimizer.zero_grad()\n","        loss = F.mse_loss(\n","            model(data_fin.x, data_fin.edge_index)[data_fin.train_mask],\n","            data_fin.y[data_fin.train_mask]\n","        )\n","        loss.backward()\n","        optimizer.step()\n","    model.eval()\n","    with torch.no_grad():\n","        preds_all = model(data_fin.x, data_fin.edge_index).cpu().numpy().flatten()\n","    gdf[f'NO2_pred_{name}'] = preds_all\n","\n","# 7.5) Save final GeoJSON with predictions\n","gdf = gdf.drop(columns='centroid')\n","out_fp = '/content/drive/MyDrive/Universiteit Utrecht/Thesis/outputs/road_segments_with_predictions_optuna.geojson'\n","gdf.to_file(out_fp, driver='GeoJSON')\n","print(\"✅ All predictions saved to:\", out_fp)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}